import torch
from langchain_community.llms import Ollama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub

import pandas as pd

from django.test import TestCase
import unittest

from .rag_evaluator import RAGEvaluator

import warnings
warnings.filterwarnings('ignore')

# knowledge_df = pd.read_csv('knowledgebase.csv')
# knowledge_base = KnowledgeBase.from_pandas(knowledge_df, columns=["content", "metadata"])
# loaded_testset = QATestset.load('textbook_testset.jsonl')
# main_df = pd.read_csv('shrinked_data.csv') 

# Create your tests here.

llama = "llama3"
gemma = "gemma:2b"
qwen = "qwen:4b"

llm = Ollama(model=llama)

evaluator = RAGEvaluator()

device = "cuda" if torch.cuda.is_available() else "cpu"

model_name = "sentence-transformers/all-mpnet-base-v2"
model_kwargs = {"device": device}

embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs
) 

prompt = hub.pull("rlm/rag-prompt")

vector_db = Chroma(persist_directory="chroma_db", embedding_function=embeddings)

retriever = vector_db.as_retriever(search_type="mmr")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Define the RAG chain
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

class TestRetriever(unittest.TestCase):
    def test_retrieve_documents(self):
        query = "What is motion?"
        retrieved_docs = retriever.invoke(query)
        
        self.assertGreater(len(retrieved_docs), 0, "No documents retrieved.")
        self.assertIn("motion", retrieved_docs[0].page_content.lower(), "Top document not relevant.")
        print("Retriever Passed ✔...")

class TestAnswerGeneration(unittest.TestCase):
    def test_generate_answer(self):
        query = "What is accelaration due to gravity?"
        context = ["acceleration due to gravity, g = 9.8 meters per second squared (m/s²)."]
        
        answer = llm.generate([query], context)
        answer_text = answer.generations[0][0].text
        self.assertTrue(answer, "No answer generated.")
        self.assertIn("acceleration due to gravity", answer_text.lower(), "Answer does not address the question.")
        print("LLM Passed ✔...")

# print("Testing RAG Pipeline...")
class TestRAGPipeline(unittest.TestCase):
    def setUp(self):
        self.pipeline = rag_chain

    def test_pipeline_integration(self):
        query = "What is accelaration due to gravity?"
        result = self.pipeline.invoke(query)
        retrieved_docs = retriever.invoke(query)

        self.assertTrue(result, "No answer generated by the pipeline.")
      
        self.assertTrue(any("acceleration due to gravity" in doc.page_content for doc in retrieved_docs), 
                        "Relevant context not retrieved.")
      
        self.assertIn("acceleration", result.lower(), "Answer does not address acceleration as expected.")
        print("RAG Pipeline Passed ✔...")

class TestRAGEvaluator(unittest.TestCase):
    def setUp(self):
        self.evaluator = RAGEvaluator()
        self.question = "When a DC armature rotates, what type of current flows through an external circuit connected to two slip rings, and how does the magnitude of this current relate to the rotation's intensity and speed?"
        self.response = rag_chain.invoke(self.question)  
        self.reference = "When a DC armature rotates, alternating current flows through the external circuit; the magnitude of current depends on the speed and intensity of rotation."

    def test_bleu_score(self):
        candidates = [self.response]
        references = [self.reference]
        bleu_score, _ = self.evaluator.evaluate_bleu_rouge(candidates, references)
        print(f"BLEU Score: {bleu_score}")
        self.assertGreaterEqual(bleu_score, 0, "BLEU score should be non-negative")
        print("Passed ✔...")

    def test_rouge1_score(self):
        candidates = [self.response]
        references = [self.reference]
        _, rouge1 = self.evaluator.evaluate_bleu_rouge(candidates, references)
        print(f"ROUGE-1 Score: {rouge1}")
        self.assertGreaterEqual(rouge1, 0, "ROUGE-1 score should be non-negative")
        print("Passed ✔...")

    def test_bert_score(self):
        candidates = [self.response]
        references = [self.reference]
        bert_p, bert_r, bert_f1 = self.evaluator.evaluate_bert_score(candidates, references)
        print(f"BERT Precision: {bert_p}, Recall: {bert_r}, F1: {bert_f1}")
        self.assertGreaterEqual(bert_p, 0, "BERT Precision should be non-negative")
        self.assertGreaterEqual(bert_r, 0, "BERT Recall should be non-negative")
        self.assertGreaterEqual(bert_f1, 0, "BERT F1 should be non-negative")
        print("Passed ✔...")

    def test_diversity_score(self):
        candidates = [self.response]
        diversity_score = self.evaluator.evaluate_diversity(candidates)
        print(f"Diversity Score: {diversity_score}")
        self.assertGreaterEqual(diversity_score, 0, "Diversity score should be non-negative")
        self.assertLessEqual(diversity_score, 1, "Diversity score should not exceed 1")
        print("Passed ✔...")

    def test_full_evaluation(self):
        metrics = self.evaluator.evaluate_all(self.question, self.response, self.reference)
        print("Full Evaluation Metrics:")
        for metric, value in metrics.items():
            print(f"{metric}: {value}")
            self.assertGreaterEqual(value, 0, f"{metric} should be non-negative")

        print("Passed ✔...")
        
if __name__ == "__main__":
    unittest.main()