import torch
from langchain_community.llms import Ollama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub

from django.test import TestCase
import unittest

import warnings
warnings.filterwarnings('ignore')

# Create your tests here.

llama = "llama3"
gemma = "gemma:2b"
qwen = "qwen:4b"

llm = Ollama(model=llama)

device = "cuda" if torch.cuda.is_available() else "cpu"

model_name = "sentence-transformers/all-mpnet-base-v2"
model_kwargs = {"device": device}

embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs
) 

prompt = hub.pull("rlm/rag-prompt")

vector_db = Chroma(persist_directory="chroma_db", embedding_function=embeddings)

retriever = vector_db.as_retriever(search_type="mmr")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Define the RAG chain
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

class TestRetriever(unittest.TestCase):
    def test_retrieve_documents(self):
        query = "What is motion?"
        retrieved_docs = retriever.invoke(query)
        
        self.assertGreater(len(retrieved_docs), 0, "No documents retrieved.")
        self.assertIn("motion", retrieved_docs[0].page_content.lower(), "Top document not relevant.")

class TestAnswerGeneration(unittest.TestCase):
    def test_generate_answer(self):
        query = "What is accelaration due to gravity?"
        context = ["acceleration due to gravity, g = 9.8 meters per second squared (m/sÂ²)."]
        
        answer = llm.generate([query], context)
        answer_text = answer.generations[0][0].text
        self.assertTrue(answer, "No answer generated.")
        self.assertIn("acceleration due to gravity", answer_text.lower(), "Answer does not address the question.")

class TestRAGPipeline(unittest.TestCase):
    def setUp(self):
        self.pipeline = rag_chain

    def test_pipeline_integration(self):
        query = "What are the potential risks of nuclear energy?"
        result = self.pipeline.invoke(query)
        retrieved_docs = retriever.invoke(query)

        self.assertTrue(result, "No answer generated by the pipeline.")
      
        self.assertTrue(any("nuclear energy" in doc.page_content for doc in retrieved_docs), 
                        "Relevant context not retrieved.")
      
        self.assertIn("risk", result.lower(), "Answer does not address risks as expected.")

if __name__ == "__main__":
    unittest.main()